也许有必要画一个程序框图

某个 episode 开始

1. 初始化状态 $$s_0$$初始化目标 g，目标 g 不该按照HER原文设置为某个状态，而还是 “弹目相对距离 < 3米”，这样就根本没有什么目标空间，目标就只是这个抽象的概念而已，甚至这个目标是固定的，无需更新，所以经验池里根本就不需要存 goal，还是和标准dqn的经验池一样，只是我们Hindsight地多存了一些transition。【一开始是想模仿 HER 原文一样把目标设为某个状态，但我们是没有办法穷举所有打中的状态的，可是如果不把所有可能的打中的状态都列举出来作为目标空间，那么目标空间就只是所有可能的打中的情形的子集，我们完成目标的几率比实际上可能打中的几率更低了，这会使奖励更加稀疏，因此不合适。】【此外，如果我们希望尽可能全面地列举出来打中的状态，那么就只能降低随机性，也就是飞行器不采用随机机动方式，但这样会和后面的hindsight矛盾】

2. **for** $$t=0, T-1$$
   1. 利用DQN（或者其它off-policy），step，

      > 1. 为了导弹和目标的轨迹所用的时间相同，并且要得到完整的轨迹方便后续制作虚假的打中了的transition，取消了提前结束（interrupt_flag和done_flag），每一次都一直坚持打到 horizon 用完
      >    （从 $$t=0$$ 打到 $$T-1$$，从不提前结束，全打完了才 done，所以 done 这个量从此变得基本毫无价值，即使是计算 hit_num 也不再使用它了，只能标记一下该状态是否为最终状态/可能这正是它本来的作用）
      > 2. 因为取消了提前结束，打没打中也不能再靠 done 来判断了，需要变成：遍历中，从 $$t=0, T-1$$，把每一个新状态 $$s_t$$ 中的弹目相对距离 r（也就是r[-1]）和 3米 比较（利用同一时间步的弹目状态，去计算相对距离），一旦有某一次显示打中了，给一个正的奖励，从此再也不参与这种判断，虽然horizon还在继续，但我们后续也不会再给奖励了（设置一个打没打中的标志位，打中了就置为 1，如果标志位是 1 就 continue）

   2. $$r_t\doteq r(s_t,a_t,g)$$，先判断打没打中的标志位，如果标志位为 1，直接跳到 c；如果标志位为0，判断打没打中，即相对距离是否<3米，若打中了，就给予一个正奖励，并将标志位记为 1，否则奖励为0；在经验池中存储 $$(s_t\vert\vert g,a_t,r_t,s_{t+1}\vert \vert g)$$ （标准经验回放） 

   3. 把导弹的坐标和导弹当前的动作和导弹的速度存在一起；把飞行器的坐标和飞行器的速度存在一起

3. 对于当前的这个 episode，如果上面 horizon 全部打完但仍然完全没打中（没有出现过弹目相对距离<3米的情形），那么把这些 0到T-1 每一个状态中导弹的坐标（xm,ym）提取出来，形成一个**导弹的轨迹**，然后和以前的episode中没被打中的**飞行器的轨迹相匹配**，计算对应时间步时二者的相对距离，如果在某个时间步的相对距离小于 3 米，计算此时的各种参数（共 12 维=6/原来的 + 6/弹目坐标速度 2023/5/22修改 错误的 没有必要修改状态的维度 仍然保持6维），去算当前的状态；取action为当前的action，去算下一时刻的状态；导弹在那个时间步下的制导律作为动作；正值作为奖励；同时计算下一个时刻二者的状态作为next state；形成一个四元组作为transition存入经验池中；然后continue，继续匹配下一条飞行器的轨迹
   【但只要飞行器的机动方式、加速度、初始位置、初始速度大小方向、是固定的，那么无论多少个episode过去，飞行器在每一个episode的轨迹都是一模一样的（除非采用随机机动），因此我们必须采取随机机动，否则和以前的episode中的飞行器轨迹匹配也是毫无意义，根本不可能找到匹配的导弹轨迹和飞行器轨迹（因为如果能匹配上的话，飞行器的轨迹又从来没有变过，在当时就不可能不打中）】

4. **for** t=1,N 从经验池中采样，训练dqn（在这边直接换PER，改成存经验池的时候像PER那么存，算个TD-error？）

该episode结束

> 相对距离：
>
> 相对速度：
>
> q：
>
> q*：
>
> 导弹速度v：
>
> 弹道倾角theta_m：

相比 HER 原论文伪代码减少了一次循环，但同样采用 Hindsight 的思想

先写好方法再写流程

- [x] 修改Env 让其返回当前弹目的坐标、速度
- [ ] 存储每一条(xm,ym,vm,action)(xt,yt,vt)轨迹
- [ ] 这个虚假的状态里的脱靶量根本不重要 我们只要知道它们相距最近的时候小于3米
- [x] 增加利用虚假的轨迹 计算当前状态的函数
- [x] 在Env里 修改done 和设置新的标志位为了给奖励
- [ ] HER流程实现
- [ ] 画出每一个episode最终的脱靶量